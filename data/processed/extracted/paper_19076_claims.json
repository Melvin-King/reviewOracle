[
  {
    "id": "R1-C1",
    "topic": "Novelty",
    "sentiment": "Positive",
    "statement": "This idea is natural and novel.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "It successfully solves mathematical premises violation of previous rastering scan autoregressive model."
  },
  {
    "id": "R1-C2",
    "topic": "Significance",
    "sentiment": "Positive",
    "statement": "The power-law scaling law is interesting and encourages follow-up work to scale up models for better performance.",
    "substantiation_type": "Vague",
    "substantiation_content": "The proposed method is proven to follow scaling laws of LLM which guarantee better performance when scaling up the training process."
  },
  {
    "id": "R1-C3",
    "topic": "Experiments",
    "sentiment": "Positive",
    "statement": "The methodâ€™s performance is competitive to the diffusion model and other generative models.",
    "substantiation_type": "None",
    "substantiation_content": ""
  },
  {
    "id": "R1-C4",
    "topic": "Reproducibility",
    "sentiment": "Positive",
    "statement": "Most of the paper claims seem valid to me.",
    "substantiation_type": "None",
    "substantiation_content": ""
  },
  {
    "id": "R1-C5",
    "topic": "Reproducibility",
    "sentiment": "Negative",
    "statement": "The second stage of training VAR transformers is too short and is hard for me to fully understand how it works.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "I wonder about the details of how to generate $h_w \\times w_h$ tokens in $r_k$ parallel using k-th position embedding map. Is the embedding 1D or 2D embedding ?. How to make sure all tokens in $r_k$ are correlated to each other ?"
  },
  {
    "id": "R1-C6",
    "topic": "Experiments",
    "sentiment": "Negative",
    "statement": "I wonder if there is any motivation to choose these scales.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "The highest resolution of the scale $r_K$ is $16 \\times 16$ and there are 10 scales (1,2,3,4,5,6,8,10,13,16)."
  },
  {
    "id": "R1-C7",
    "topic": "Reproducibility",
    "sentiment": "Negative",
    "statement": "I think the paper should include the sampling algorithm of autoregressive model with hyper-parameter details such as temperature, top-k, top-p and CFG sampling.",
    "substantiation_type": "None",
    "substantiation_content": ""
  },
  {
    "id": "R1-C8",
    "topic": "Reproducibility",
    "sentiment": "Negative",
    "statement": "The zero-shot generalisation algorithm should be included in the paper for clarity.",
    "substantiation_type": "None",
    "substantiation_content": ""
  },
  {
    "id": "R1-C9",
    "topic": "Significance",
    "sentiment": "Positive",
    "statement": "The proposed method is more efficient than the traditional autoregressive model.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "requiring only $O(n^4)$ compared to $O(n^6)$ of the raster autoregressive model."
  },
  {
    "id": "R2-C1",
    "topic": "Novelty",
    "sentiment": "Positive",
    "statement": "The proposed method (VAR) is simple, intuitive and novel.",
    "substantiation_type": "Vague",
    "substantiation_content": "It is not difficult to see why it works well."
  },
  {
    "id": "R2-C2",
    "topic": "Experiments",
    "sentiment": "Positive",
    "statement": "The empirical results are strong.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "VAR demonstrates competitive performance on ImageNet in terms of generation quality, diversity and inference speed. It also demonstrates scaling laws up to 2.0B parameters."
  },
  {
    "id": "R2-C3",
    "topic": "Significance",
    "sentiment": "Positive",
    "statement": "The approach is expected to scale to foundation-model T2I systems easily.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "The approach is based on a decoder-only transformer (a tried and tested architecture for autoregressive generation)."
  },
  {
    "id": "R2-C4",
    "topic": "Writing",
    "sentiment": "Positive",
    "statement": "The paper is well written and easy to read.",
    "substantiation_type": "Vague",
    "substantiation_content": "The method/motivation/story is communicated clearly to the reader."
  },
  {
    "id": "R2-C5",
    "topic": "Experiments",
    "sentiment": "Negative",
    "statement": "The reported inference efficiency is a bit disingenuous.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "The comparison with DiT uses 250 steps, which is much more than what SotA samplers require. Moreover, diffusion models can be distilled into 1-4 step models that are even faster."
  },
  {
    "id": "R2-C6",
    "topic": "Experiments",
    "sentiment": "Negative",
    "statement": "The improvement in latency compared to raster scan autoregressive models seems to be due to better use of parallel resources, and for larger batches/measuring throughput this advantage may fade.",
    "substantiation_type": "Vague",
    "substantiation_content": "However, for larger batches/measuring throughput this advantage may fade."
  },
  {
    "id": "R2-C7",
    "topic": "Reproducibility",
    "sentiment": "Negative",
    "statement": "There are some missing details and insight, especially with regards to the multi-scale VQVAE.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "There are also details that are present in the code that really should be in the paper, such as the number of tokens per image/scale. The code provided doesn't give details to reproduce the VQVAE, only the transformer."
  },
  {
    "id": "R2-C8",
    "topic": "Experiments",
    "sentiment": "Neutral",
    "statement": "Questions about the VQVAE's reconstruction performance, scale selection, and latent space information.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "What is its reconstruction performance of the VQVAE (compared to e.g. the SDXL VAE)? How does one choose the number of scales? How many codes end up being used over the different scales and do different scales capture similar (spatial) information in the latent space vs the image space? It would also be great to see an ablation like Table 3 for the VQVAE."
  },
  {
    "id": "R2-C9",
    "topic": "Writing",
    "sentiment": "Negative",
    "statement": "The use of 'zero-shot' to refer to the model's editing ability is different in nature to zero-shot generalisation in LLMs, so the link made in the paper is a little disingenuous.",
    "substantiation_type": "Vague",
    "substantiation_content": "The use of 'zero-shot' is different in nature to zero-shot generalisation in LLMs."
  },
  {
    "id": "R2-C10",
    "topic": "Experiments",
    "sentiment": "Negative",
    "statement": "The editing performance doesn't seem to be very strong.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "Inpainting generation spilling outside of the box."
  },
  {
    "id": "R2-C11",
    "topic": "Reproducibility",
    "sentiment": "Negative",
    "statement": "The paper doesn't give details on how the editing is performed.",
    "substantiation_type": "None",
    "substantiation_content": ""
  },
  {
    "id": "R3-C1",
    "topic": "Novelty",
    "sentiment": "Positive",
    "statement": "The paper introduces Visual AutoRegressive modeling (VAR), which uses a coarse-to-fine approach for image generation.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "VAR drastically improves performance, reducing FID from 18.65 to 1.73 and increasing IS from 80.4 to 350.2, with 20x faster inference. It outperforms diffusion transformers in quality, speed, efficiency, and scalability."
  },
  {
    "id": "R3-C2",
    "topic": "Significance",
    "sentiment": "Positive",
    "statement": "VAR models show scaling laws like large language models and demonstrate zero-shot generalization in image editing tasks.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "VAR drastically improves performance, reducing FID from 18.65 to 1.73 and increasing IS from 80.4 to 350.2, with 20x faster inference. It outperforms diffusion transformers in quality, speed, efficiency, and scalability."
  },
  {
    "id": "R3-C3",
    "topic": "Novelty",
    "sentiment": "Positive",
    "statement": "The exploration of autoregression in visual generation is indeed a worthy topic.",
    "substantiation_type": "Vague",
    "substantiation_content": "The scale in vision signals is a natural choice for autoregressive generation."
  },
  {
    "id": "R3-C4",
    "topic": "Novelty",
    "sentiment": "Positive",
    "statement": "This work is the first to explore a visual generative framework using a multi-scale autoregressive paradigm with next-scale prediction.",
    "substantiation_type": "None",
    "substantiation_content": ""
  },
  {
    "id": "R3-C5",
    "topic": "Experiments",
    "sentiment": "Positive",
    "statement": "The paper demonstrates significant advancements in visual autoregressive model performance, with GPT-style autoregressive methods surpassing strong diffusion models in image synthesis for the first time.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "VAR drastically improves performance, reducing FID from 18.65 to 1.73 and increasing IS from 80.4 to 350.2, with 20x faster inference. It outperforms diffusion transformers in quality, speed, efficiency, and scalability."
  },
  {
    "id": "R3-C6",
    "topic": "Significance",
    "sentiment": "Positive",
    "statement": "The paper presents a promising scaling law for the proposed visual autoregressive modeling paradigm.",
    "substantiation_type": "Vague",
    "substantiation_content": "VAR models show scaling laws like large language models."
  },
  {
    "id": "R3-C7",
    "topic": "Experiments",
    "sentiment": "Negative",
    "statement": "There is no ablation study on the newly proposed VQ-VAE model.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "In Table 3, the performance differences between the first two rows cannot be solely attributed to the model change from AR to VAR, as the VQ-VAE model has also been modified."
  },
  {
    "id": "R3-C8",
    "topic": "Novelty",
    "sentiment": "Negative",
    "statement": "The resolutions for VAR generation appear to be pre-defined and bound to the VQ-VAE model during its pre-training. Adjusting the number of resolutions or maximum resolution without re-training the VQ-VAE model seems non-trivial.",
    "substantiation_type": "None",
    "substantiation_content": ""
  },
  {
    "id": "R3-C9",
    "topic": "Limitations",
    "sentiment": "Neutral",
    "statement": "This paper validates the effectiveness of VAR only in class-conditional generation scenarios. Applying VAR to text-to-image generation is a worthwhile area for future exploration.",
    "substantiation_type": "None",
    "substantiation_content": ""
  },
  {
    "id": "R4-C1",
    "topic": "Novelty",
    "sentiment": "Positive",
    "statement": "The paper introduces VAR, a novel autoregressive generative model for images that treats each scale in a multi-resolution feature pyramid as a token.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "Unlike traditional models that predict the next token from a rasterized grid, VAR predicts the next scale in a multi-resolution grid."
  },
  {
    "id": "R4-C2",
    "topic": "Significance",
    "sentiment": "Positive",
    "statement": "This approach demonstrates greater scalability than next-token prediction and extends the well-known scaling laws from language modeling to image generation.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "Extensive experiments show that VAR outperforms both diffusion and AR baselines while offering improved efficiency in both training and inference."
  },
  {
    "id": "R4-C3",
    "topic": "Significance",
    "sentiment": "Positive",
    "statement": "The paper addresses the significant question of bridging the performance gap between autoregressive language models and autoregressive image generation.",
    "substantiation_type": "Vague",
    "substantiation_content": "This makes the topic highly relevant for the community and potentially impactful."
  },
  {
    "id": "R4-C4",
    "topic": "Novelty",
    "sentiment": "Positive",
    "statement": "The method is well-motivated and utilizes well-known building blocks from LLMs.",
    "substantiation_type": "Vague",
    "substantiation_content": "Hence, VAR is an important step toward showing that with a proper scheme, widely used LLM architectures can perform competitively in the Image domain."
  },
  {
    "id": "R4-C5",
    "topic": "Experiments",
    "sentiment": "Positive",
    "statement": "The experimental section is comprehensive, demonstrating VAR's performance and efficiency in image generation on ImageNet 256 and 512.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "Additionally, the authors provide in-depth discussion of scaling laws for VAR."
  },
  {
    "id": "R4-C6",
    "topic": "Experiments",
    "sentiment": "Positive",
    "statement": "Ablation studies in Appendix D clearly illustrate the contribution of different aspects of VAR to the final model.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "Ablation studies in Appendix D"
  },
  {
    "id": "R4-C7",
    "topic": "Writing",
    "sentiment": "Negative",
    "statement": "While the writing of the paper is clear for the most part, the method section could benefit from better presentation.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "I found section 3, especially section 3.2, slightly confusing. I suggest that the authors add more details and clarification on VAR's training process, the residual tokenization, and the workings of the transformer part. Currently, these details are somewhat obscured in Algorithm 1 and 2, and Figure 4."
  },
  {
    "id": "R4-C8",
    "topic": "Reproducibility",
    "sentiment": "Negative",
    "statement": "Some claims in the paper are slightly exaggerated.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "For example, in the abstract, the authors mention that VAR brings the FID from 18.65 to 1.73. While this is true, the FID of 18.65 belongs to a relatively weak baseline for AR models. It would be better to rewrite such claims in relation to more realistic baselines, such as RQ-VAE models, which are closer to VAR's methodology"
  },
  {
    "id": "R4-C9",
    "topic": "Experiments",
    "sentiment": "Negative",
    "statement": "The baselines used for the diffusion part are also relatively weak.",
    "substantiation_type": "Specific_Citation",
    "substantiation_content": "For instance, MDTv2 [1] is a transformer-based diffusion model that achieves an FID of 1.58 on ImageNet 256. Therefore, it would be more appropriate to state that VAR performs 'competitively' with diffusion models rather than significantly outperforms them."
  }
]